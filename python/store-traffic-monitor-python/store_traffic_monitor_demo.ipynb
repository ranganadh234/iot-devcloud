{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Traffic Monitor\n",
    "\n",
    "Store traffic monitor is an advanced use-case example, which builds on the two previous examples (<a href=\"../flaw-detector-python/flawdetector-demo.ipynb\">flaw detector</a> and <a href=\"../object-detection-python/object_detection_demo.ipynb\">object detector</a>) and shows how a user can perform object detection and inference on multiple videos simultaneously. In this use case, the application monitors the activity of people inside and outside an imaginary store as well as keeps track of product inventory using a pre-trained neural network for detection. This application is capable of detecting objects on any number of screens as long as sufficient compute power is available.\n",
    "\n",
    "This demo assumes you have already tried the <a href=\"../object-detection-python/object_detection_demo.ipynb\">object detector</a> sample. If you have not done so already, we recommend going through that sample first to understand the workflow for developing a deep learning application using OpenVINO. \n",
    "\n",
    "More information on using the store traffic monitor, as well as the original source code, can be found on GitHub:\n",
    "\n",
    "https://github.com/intel-iot-devkit/store-traffic-monitor-python\n",
    "\n",
    "Note that the code in this notebook has been slightly modified from the version on GitHub in order to work in the IoT DevCloud environment.\n",
    "\n",
    "## Overview of How it works?\n",
    "\n",
    "The counter uses the inference engine included in the OpenVINO™ toolkit. A trained neural network detects objects within a designated area and displays a green bounding box over them. This reference implementation identifies multiple intruding objects entering the frame and identifies their class, count, and time entered. \n",
    "\n",
    "At start-up, the sample application reads a configuration file and loads a network and images from the video input to the Inference Engine (IE) plugin. At runtime, computations are offloaded to a the hardware accelerator (Intel® Core CPU, Intel® HD Graphics GPU, Intel® Xeon CPU, Intel® Movidius™ and/or Neural Compute Stick). Once the inference is completed, the output videos are appropriately rendered and stored in the /results directory, which can then be viewed within the Jupyter Notebook instance\n",
    "\n",
    "## Demonstration objectives\n",
    "* Video as input is supported using **OpenCV**\n",
    "* Inference performed on edge compute nodes with various target compute devices\n",
    "* **OpenCV** provides the bounding boxes, labels and other information\n",
    "* Visualization of the resulting bounding boxes\n",
    "* Demonstrate the Async API in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup\n",
    "\n",
    "Import dependencies for the notebook by running the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
    "from demoTools.demoutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Advanced OpenVINO\n",
    "\n",
    "The complete listing of source code for this example is in <a href=\"store_traffic_monitor.py\">store_traffic_monitor.py</a>.\n",
    "This application takes multiple video streams as inputs. The application is designed to work in both sync and async mode. In sync mode, the processing is done by the compute device by taking one frame at a time, whereas in async mode, the device takes multiple frames and processes them in parallel.\n",
    "\n",
    "\n",
    "**Configure the Application**\n",
    "\n",
    "All the configurations are written to store-traffic-monitor-python/conf_fp*.txt. We have two configuration files: conf_fp16.txt and conf_fp32.txt  \n",
    "\n",
    "- **conf_fp16.txt**: use if you want the inference to be run on Intel® Movidius™ Neural Compute Stick as it offers native FP16 precision.\n",
    "- **conf_fp32.txt**: use if you want the inference to be run on Intel® Core CPU, Intel® HD Graphics GPU and Intel® Xeon CPU.\n",
    "\n",
    "1st line: path/to/model.xml -> \n",
    "This is the path to the model topology in the local file system.\n",
    "The model topology file is the .xml file that the model optimizer produces, containing the IR of the model's topology.\n",
    "\n",
    "2nd line: path/to/model.bin ->\n",
    "This is the path to the model weights in the local file system.\n",
    "The model weights file is the .bin file that the model optimizer produces, containing the IR of the model's weights.\n",
    "\n",
    "3rd line: path/to/labels ->\n",
    "This is a path to the labels file in the local file system.\n",
    "The labels file is a text file containing all the classes/labels that the model can recognize, in the order that it was trained to recognize them (one class per line). All detection models work with integer labels and not string labels (e.g. for the mobilenet-ssd model, the **number 15 represents the class \"person\"**).\n",
    "For the mobilenet-ssd model, we provide the class file labels.txt in the **\"/data/reference-sample-data/store-traffic-monitor-python\"** folder.\n",
    "\n",
    "**Note** : store_traffic_monitor application is used to detect a person and a bottle in a frame. For the mobilenet-ssd model used in this example, the line number 5 and  line number 15 in the labels.txt file correspond to bottle and person, respectively. If you are using a different model, then the items in labels.txt file would need to be adjusted appropriately.\n",
    "\n",
    "Each of the following lines contain the path/to/video followed by the label to be detected on that video, e.g.:\n",
    "\n",
    "**/data/reference-sample-data/store-traffic-monitor-python/people-detection.mp4** person\n",
    "\n",
    "The path/to/video is the path, in the local file system, to a video that is used as input. The labels used must coincide with the labels from the labels file.\n",
    "\n",
    "**Models to Use ?**\n",
    "\n",
    "The application works with any object-detection model, provided it has the same input and output format of the mobilenet-ssd model.\n",
    "The model can be any object detection model:\n",
    "\n",
    "* that is provided by OpenVINO™ toolkit.\n",
    "These can be found in the openvino/deployment_tools/intel_models folder.\n",
    "* downloaded using the model downloader.\n",
    "/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name <model_name> -o <output_dir>\n",
    "* created by the user\n",
    "\n",
    "For first-time use, we recommend using the mobilenet-ssd model provided as a part of this application. The models can be found under the folder \"/data/reference-sample-data/models/mobilenet-ssd\"\n",
    "\n",
    "**Videos to Use ?**\n",
    "\n",
    "The application works with any input video. Sample videos for object detection are provided here.\n",
    "\n",
    "For first-use, we recommend using the people-detection, one-by-one-person-detection, bottle-detection videos.\n",
    "E.g.:\n",
    "\n",
    "/data/reference-sample-data/store-traffic-monitor-python/people-detection.mp4 person<br>\n",
    "/data/reference-sample-data/store-traffic-monitor-python/one-by-one-person-detection.mp4 person<br>\n",
    "/data/reference-sample-data/store-traffic-monitor-python/bottle-detection.mp4 bottle\n",
    "\n",
    "For the sake of simplicity, the videos are copied into /data/reference-sample-data/store-traffic-monitor-python/ folder\n",
    "\n",
    "**Command line arguments options and how they are interpreted in the application source code**\n",
    "\n",
    "```python\n",
    "** python3 store_traffic_monitor.py -d CPU -c conf_fp32.txt -o results **\n",
    "```\n",
    "\n",
    "#### The description of the arguments used in the argument parser is the command line executable equivalent.\n",
    "\n",
    " *  -o  location where the output file with inference results needs to be stored. (results/core or results/xeon or results/gpu or results/myriad)\n",
    "\n",
    " *  -d Type of Hardware Acceleration (CPU or GPU or MYRIAD or HDDL)\n",
    " *  -c configuration file to use (\"conf_fp32.txt\" for Intel® Core CPU, Intel® HD Graphics GPU and Intel® Xeon CPU. \"conf_fp16.txt\" for Intel® Movidius™ Neural Compute Stick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Choosing Device\n",
    "First, we must select the device used for inferencing. This is done by loading the appropriate plugin to initialize the specified device (CPU, GPU, MYRIAD etc) and load the extensions library (if specified) provided in the extension/ folder for the device.\n",
    "\n",
    "The plugin class is IEPlugin and can be constructed as follows:\n",
    "```python\n",
    "    # Parses the configuration file \"conf.txt\" and reads model_xml, model_bin, labels_file, videoCaps\n",
    "    parse_conf_file()\n",
    "    # Plugin initialization for specified device and load extensions library\n",
    "    print(\"Initializing plugin for {} device...\".format(TARGET_DEVICE))\n",
    "    plugin = IEPlugin(device=TARGET_DEVICE)\n",
    "    if 'CPU' in TARGET_DEVICE:\n",
    "        plugin.add_cpu_extension(CPU_EXTENSION)\n",
    "```\n",
    "\n",
    "**Note**: Currently, three types of plugins are supported: CPU, GPU,HDDL and MYRIAD. The CPU plugin may require additional extensions to improve performance. Use add_cpu_extension function to load these additional extensions.\n",
    "\n",
    "### 1.2 Read the IR (Intermediate Representation) model\n",
    "\n",
    "Intel Model Optimizer creates Intermediate Representation (IR) models that are optimized for different Intel hardware.\n",
    "We can import these optimized models (weights) into our neural network using **`IENetwork`**. \n",
    "```python\n",
    "    net = IENetwork.from_ir(model=model_xml, weights=model_bin)\n",
    "    assert len(net.inputs.keys()) == 1, \"Sample supports only single input topologies\"\n",
    "    assert len(net.outputs) == 1, \"Sample supports only single output topologies\"\n",
    "    input_blob = next(iter(net.inputs))\n",
    "    out_blob = next(iter(net.outputs))\n",
    "```\n",
    "\n",
    "### 1.3 Load the network into the plugin\n",
    "\n",
    "Once we have the plugin and the network, we can load the IR into the plugin using **`plugin.load`**.\n",
    "\n",
    "```python\n",
    "    # Loading IR model to the plugin.\n",
    "    exec_net = plugin.load(network=net, num_requests=2)\n",
    "    # Read and pre-process input image\n",
    "    n, c, h, w = net.inputs[input_blob]\n",
    "    del net\n",
    "\n",
    "```\n",
    "\n",
    "### 1.4 Start video capture using OpenCV \n",
    "\n",
    "Now we are ready to capture the frames from the video sample using **OpenCV VideoCapture** API.\n",
    "Upon getting the frame we are ready to perform inference.\n",
    "\n",
    "```python\n",
    "cap = cv2.VideoCapture(input_stream)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.5 Prepare the Model\n",
    "\n",
    "Run the cells below to download the mobilenet-ssd model and convert it using the Intel Model Optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --print_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\" # export OPENBLAS_NUM_THREADS=4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name mobilenet-ssd -o raw_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model raw_models/public/mobilenet-ssd/mobilenet-ssd.caffemodel \\\n",
    "--data_type FP32 \\\n",
    "-o models/mobilenet-ssd/FP32 \\\n",
    "--scale 256 \\\n",
    "--mean_values [127,127,127] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model raw_models/public/mobilenet-ssd/mobilenet-ssd.caffemodel \\\n",
    "--data_type FP16 \\\n",
    "-o models/mobilenet-ssd/FP16 \\\n",
    "--scale 256 \\\n",
    "--mean_values [127,127,127] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Running on the IoT DevCloud\n",
    "\n",
    "### 2.1 Creating job file\n",
    "\n",
    "Like in the object detection example, we will perform inference on multiple edge compute devices by submitting jobs into the IoT DevCloud queue. To do this, first, we need to create a job file.\n",
    "The job file is a Bash script, and will be executed on the edge compute node.\n",
    "For this example, we have written the job file for you in the notebook.\n",
    "Run the following cell to write this in to the file \"store_traffic_manager_job.sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile store_traffic_monitor_job.sh\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "OUTPUT_FILE=$1\n",
    "DEVICE=$2\n",
    "FP_MODEL=$3\n",
    "NUM_VIDEOS=$4\n",
    "NIREQ=$5\n",
    "\n",
    "# Traffic monitor script writes output to a file inside a directory. We make sure that this directory exists.\n",
    "#  The output directory is the first argument of the bash script\n",
    "mkdir -p $1\n",
    "\n",
    "if [ \"$DEVICE\" = \"HETERO:FPGA,CPU\" ]; then\n",
    "    #source /opt/fpga_support_files/setup_env.sh\n",
    "    # Changed aocx file to R3\n",
    "    source /opt/intel/init_openvino.sh\n",
    "    aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/2019R3_PV_PL1_FP16_MobileNet_Clamp.aocx\n",
    "fi\n",
    "\n",
    "if [ \"$FP_MODEL\" = \"FP32\" ]; then\n",
    "    config_file=\"conf_fp32.txt\"  \n",
    "else\n",
    "    config_file=\"conf_fp16.txt\"\n",
    "fi\n",
    "\n",
    "SAMPLEPATH=$PBS_O_WORKDIR\n",
    "# Traffic monitor takes 3 inputs, which are passed in as arguments to the bash script. \n",
    "#  -o : output directory of the videos\n",
    "#  -d : device to use for inference\n",
    "#  -c : conf file to use\n",
    "#  -n : number of videos to process\n",
    "\n",
    "python3 store_traffic_monitor.py  -d $DEVICE \\\n",
    "                                    -m ${SAMPLEPATH}/models/mobilenet-ssd/${FP_MODEL}/mobilenet-ssd.xml \\\n",
    "                                    -l labels.txt \\\n",
    "                                    -e /opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_avx2.so \\\n",
    "                                    -lp false \\\n",
    "                                    -o $OUTPUT_FILE \\\n",
    "                                    -c $config_file \\\n",
    "                                    -n $NUM_VIDEOS \\\n",
    "                                    -nireq $NIREQ\n",
    "\n",
    "g++ -std=c++14 ROI_writer.cpp -o ROI_writer -lopencv_core -lopencv_videoio -lopencv_imgproc \\\n",
    "-lopencv_highgui -lopencv_imgcodecs -fopenmp -I/opt/intel/openvino/opencv/include/ \\\n",
    "-L/opt/intel/openvino/opencv/lib/\n",
    "\n",
    "./ROI_writer $config_file $OUTPUT_FILE 1 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Submitting to Job Queue\n",
    "\n",
    "Now that we have the job script, we can submit the jobs to compute nodes using the `qsub` command.\n",
    "Let's try submitting the store_traffic_monitor job to 5 different types of nodes.\n",
    "\n",
    "There are three options of `qsub` command that we use for this:\n",
    "- `-l` : this option lets us select the number and the type of nodes using `nodes={node_count}:{property}`. \n",
    "- `-F` : this option lets us send arguments to the bash script. \n",
    "- `-N` : this option lets us name the job so that it is easier to distinguish between them.\n",
    "\n",
    "To see the available types of nodes (e.g. properties) on the IoT DevCloud, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes.\n",
    "\n",
    "Now let's actually submit the jobs. \n",
    "The 5 code cells below will submit jobs to nodes with different architectures.\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "#### Submitting to an edge compute node with an Intel® CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel® Core™ i5-6500TE processor</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Core CPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_core =!qsub store_traffic_monitor_job.sh -l nodes=1:idc001skl:i5-6500te -F \"results/core CPU FP32 3 2\" -N monitor_core\n",
    "print(job_id_core[0])\n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/core', 'i_progress_'+job_id_core[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/core', 'post_progress_'+job_id_core[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® Xeon® CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz-\">Intel® \n",
    "    Xeon® Processor E3-1268L v5</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Xeon CPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_xeon = !qsub store_traffic_monitor_job.sh -l nodes=1:idc007xv5:e3-1268l-v5 -F \"results/xeon CPU FP32 3 2\" -N monitor_xeon\n",
    "print(job_id_xeon[0])\n",
    "if job_id_xeon:\n",
    "    progressIndicator('results/xeon', 'i_progress_'+job_id_xeon[0]+'.txt', \"Inference\", 0, 100)  \n",
    "    progressIndicator('results/xeon', 'post_progress_'+job_id_xeon[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® Core CPU and using the onboard Intel® GPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel® Core i5-6500TE</a>. The inference workload will run on the Intel® HD Graphics 530 card integrated with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Core CPU and an Intel GPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub store_traffic_monitor_job.sh -l nodes=1:idc001skl:intel-hd-530 -F \"results/gpu GPU FP32 3 2\" -N monitor_gpu\n",
    "print(job_id_gpu[0])\n",
    "#Progress indicators\n",
    "if job_id_gpu:\n",
    "    progressIndicator('results/gpu', 'i_progress_'+job_id_gpu[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/gpu', 'post_progress_'+job_id_gpu[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® NCS 2 (Neural Compute Stick 2)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a>. The inference workload will run on an <a \n",
    "    href=\"https://software.intel.com/en-us/neural-compute-stick\">Intel Neural Compute Stick 2</a> installed in this  node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with Intel Movidius NCS 2...\")\n",
    "#Submit job to the queue\n",
    "job_id_myriad = !qsub store_traffic_monitor_job.sh -l nodes=1:idc004nc2:intel-ncs2 -F \"results/myriad MYRIAD FP16 3 2\" -N monitor_myriad\n",
    "print(job_id_myriad[0])\n",
    "#Progress indicators\n",
    "if job_id_myriad:\n",
    "    progressIndicator('results/myriad', 'i_progress_'+job_id_myriad[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/myriad', 'post_progress_'+job_id_myriad[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IEI Mustang-V100-MX8 ( Intel® Movidius™ Myriad™ X Vision Processing Unit (VPU))\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a>. The inference workload will run on an <a \n",
    "    href=\"https://www.ieiworld.com/mustang-v100/en/\">IEI Mustang-V100-MX8 </a>accelerator installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with Intel HDDL-R...\")\n",
    "#Submit job to the queue\n",
    "job_id_hddl = !qsub store_traffic_monitor_job.sh -l nodes=1:idc002mx8:iei-mustang-v100-mx8 -F \"results/hddl HDDL FP32 3 128\" -N monitor_hddl\n",
    "print(job_id_hddl[0])\n",
    "#Progress indicators\n",
    "if job_id_hddl:\n",
    "    progressIndicator('results/hddl', 'i_progress_'+job_id_hddl[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/hddl', 'post_progress_'+job_id_hddl[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with UP Squared Grove IoT Development Kit (UP2)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/up-squared-grove-dev-kit\">UP Squared Grove IoT Development Kit</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/96488/Intel-Atom-x7-E3950-Processor-2M-Cache-up-to-2-00-GHz-\">Intel® Atom® x7-E3950 Processor</a>. The inference  workload will run on the integrated Intel® HD Graphics 505 card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with UP Squared...\")\n",
    "#Submit job to the queue\n",
    "job_id_up2 = !qsub store_traffic_monitor_job.sh -l nodes=1:up-squared -F \"results/up2/ GPU FP32 3 2\" -N monitor_up2\n",
    "print(job_id_up2[0]) \n",
    "#Progress indicators\n",
    "if job_id_up2:\n",
    "    progressIndicator('results/up2', 'i_progress_'+job_id_up2[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/up2', 'post_progress_'+job_id_up2[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with IEI Mustang-F100-A10 (Intel® Arria® 10 FPGA)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core™ i5-6500te CPU</a> . The inference workload will run on the <a href=\"https://www.ieiworld.com/mustang-f100/en/\"> IEI Mustang-F100-A10 </a> card installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with Intel FPGA HDDL-F...\")\n",
    "#Submit job to the queue\n",
    "job_id_fpga = !qsub store_traffic_monitor_job.sh -l nodes=1:idc003a10:iei-mustang-f100-a10 -F \"results/fpga HETERO:FPGA,CPU FP32 3 2\" -N monitor_fpga\n",
    "print(job_id_fpga[0])\n",
    "#Progress indicators\n",
    "if job_id_fpga:\n",
    "    progressIndicator('results/fpga', 'i_progress_'+job_id_fpga[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/fpga', 'post_progress_'+job_id_fpga[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Check if the jobs are done\n",
    "\n",
    "Run the following cell to bring up the qstat widget and monitor the running jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs you have submitted (referenced by `Job ID`).\n",
    "It should also show a job in the queue \"jupyterhub\", which runs the present Jupyter Notebook session. \n",
    "In the column \"S\", you can find the status of each job. \"Q\" means that the job is waiting for resources in the queue. \"R\" means that the job is running. When the a job is running, the progress indicators underneath the cells that submitedt that job will tell you how far the job has progressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wait!**\n",
    "\n",
    "Before moving to step 3, make sure that all the monitor_*  jobs submitted to the queue are completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Results\n",
    "\n",
    "Once the jobs are complete, the queue software writes the stdout and stderr streams into files with names of the form\n",
    "\n",
    "`monitor_{type}.o{JobID}`\n",
    "\n",
    "`monitor_{type}.e{JobID}`\n",
    "\n",
    "(here, the prefix `monitor_{type}` is based on our `-N` argument of `qsub`)\n",
    "\n",
    "However, for our store monitor example, rather than studying stdout/stderr, you will likely be more interested in viewing the main output in the mp4 videos that are stored in the `results/` directory.\n",
    "We wrote a short utility script that will display these videos in the notebook. Run the cells below to view the video.\n",
    "See `demoutils.py` if interested in the script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the following four cells to see the output  from out jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank (Intel® Core CPU)',\n",
    "          ['results/core/inference_output_Video_0.mp4',\n",
    "           'results/core/inference_output_Video_1.mp4', \n",
    "           'results/core/inference_output_Video_2.mp4', \n",
    "           'results/core/Statistics.mp4'],\n",
    "           'results/core/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank Xeon (Intel® Xeon CPU)',\n",
    "          ['results/xeon/inference_output_Video_0.mp4',\n",
    "           'results/xeon/inference_output_Video_1.mp4',\n",
    "           'results/xeon/inference_output_Video_2.mp4',\n",
    "           'results/xeon/Statistics.mp4'],\n",
    "           'results/xeon/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Intel GPU (Intel® Onboard GPU)',\n",
    "          ['results/gpu/inference_output_Video_0.mp4',\n",
    "           'results/gpu/inference_output_Video_1.mp4',\n",
    "           'results/gpu/inference_output_Video_2.mp4',\n",
    "           'results/gpu/Statistics.mp4'],\n",
    "           'results/gpu/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank + Myriad (Intel® Core + Movidius NCS 2)',\n",
    "          ['results/myriad/inference_output_Video_0.mp4',\n",
    "           'results/myriad/inference_output_Video_1.mp4', \n",
    "           'results/myriad/inference_output_Video_2.mp4', \n",
    "           'results/myriad/Statistics.mp4'],\n",
    "           'results/myriad/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank + Intel HDDL-R',\n",
    "          ['results/hddl/inference_output_Video_0.mp4',\n",
    "           'results/hddl/inference_output_Video_1.mp4', \n",
    "           'results/hddl/inference_output_Video_2.mp4', \n",
    "           'results/hddl/Statistics.mp4'],\n",
    "           'results/hddl/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('UP Squared Grove IoT Development Kit (UP2)',\n",
    "          ['results/up2/inference_output_Video_0.mp4',\n",
    "           'results/up2/inference_output_Video_1.mp4',\n",
    "           'results/up2/inference_output_Video_2.mp4',\n",
    "           'results/up2/Statistics.mp4'],\n",
    "           'results/up2/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank + Intel FPGA HDDL-F',\n",
    "          ['results/fpga/inference_output_Video_0.mp4',\n",
    "           'results/fpga/inference_output_Video_1.mp4',\n",
    "           'results/fpga/inference_output_Video_2.mp4',\n",
    "           'results/fpga/Statistics.mp4'],\n",
    "           'results/fpga/stats.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Assess Performance\n",
    "\n",
    "The running time of each inference task is recorded in `results/*/stats.txt`, where the subdirectory name corresponds to the architecture of the target edge compute node. Run the cell below to plot the results of all jobs side-by-side. Lower values mean better performance. Keep in mind that some architectures are optimized for the highest performance, others for low power or other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_list = [('core', 'Intel Core\\ni5-6500TE\\nCPU'),\n",
    "             ('xeon', 'Intel Xeon\\nE3-1268L v5\\nCPU'),\n",
    "             ('gpu', ' Intel Core\\ni5-6500TE\\nGPU'),\n",
    "             ('fpga', ' IEI Mustang\\nF100-A10\\nFPGA'),\n",
    "             ('myriad', 'Intel\\nNCS2'),\n",
    "             ('up2', 'Intel Atom\\nx7-E3950\\nUP2/GPU'),\n",
    "             ('hddl', 'IEI Mustang\\nV100-MX8\\nVPU')]\n",
    "\n",
    "stats_list = []\n",
    "for arch, a_name in arch_list:\n",
    "    if 'job_id_'+arch in vars():\n",
    "        stats_list.append(('results/{arch}/stats.txt'.format(arch=arch), a_name))\n",
    "    else:\n",
    "        stats_list.append(('placeholder'+arch, a_name))\n",
    "\n",
    "summaryPlot(stats_list, 'Architecture', 'Time, seconds', 'Inference Engine Processing Time', 'time')\n",
    "summaryPlot(stats_list, 'Architecture', 'Frames per second', 'Inference Engine FPS', 'fps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
